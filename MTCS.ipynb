{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgQ9YR6hF0qPCKD11ouxms"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- 일단 중복 고려 안함. 이동 시 발생하는 reward 0.\n","- 오직 게임이 끝나고 win, lose, draw에 따라서만 reward 발생\n","- MTCS 구현\n","- [MTCS 코드 레퍼런스](https://cafe.daum.net/oracleoracle/SSSv/9?q=몬테카를로%20트리%20탐색)"],"metadata":{"id":"NEQfh9B2ofMw"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"KWFBBg-ZjXOf","executionInfo":{"status":"ok","timestamp":1728281793183,"user_tz":-540,"elapsed":7560,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"outputs":[],"source":["import time\n","import os\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from typing import Tuple\n","from collections import deque\n","import copy\n","from scipy.special import softmax\n","import random\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"markdown","source":["# 틱택토 환경"],"metadata":{"id":"nZPH06v7jkS3"}},{"cell_type":"markdown","source":["- player = True : 컴퓨터 ('X')\n","- player = False : 상대방 ('O')"],"metadata":{"id":"aTGToHmiofDN"}},{"cell_type":"code","source":["class Environment:\n","    def __init__(self):\n","        self.n = 3\n","        self.num_actions = self.n**2\n","        self.present_state = np.zeros((self.n, self.n))\n","        self.action_space = np.arange(self.num_actions)\n","        self.available_actions = np.ones(self.num_actions)\n","        self.reward_dict = {'win':1, 'lose':-1, 'draw': -0.1, 'good_action':0, 'overlapped':0}\n","        self.done = False\n","\n","\n","    def step(self, action_idx:int, player:bool):\n","        '''\n","        에이전트가 선택한 action에 따라 주어지는 next_state, reward, done\n","        '''\n","        x, y = np.divmod(action_idx, self.n)\n","\n","        self.present_state[x,y] = player*2 -1\n","        next_state = self.present_state\n","        done, is_win = self.is_done(next_state)\n","        reward = self.reward_dict['good_action']\n","        self.available_actions = self.check_available_action(self.present_state)\n","\n","        if done:\n","            if is_win == \"win\":\n","                reward = self.reward_dict['win']\n","            elif is_win == \"lose\":\n","                reward = self.reward_dict['lose']\n","            else:\n","                reward = self.reward_dict['draw']\n","\n","        self.done = done\n","\n","        return next_state, reward, done, is_win\n","\n","\n","    def reset(self):\n","        '''\n","        게임판 초기화\n","        '''\n","        self.present_state = np.zeros((self.n, self.n))\n","        self.available_actions = np.ones(self.num_actions)\n","        self.done = False\n","\n","\n","    def render(self):\n","        '''\n","        print the current state\n","        '''\n","        render_state = np.array([['.','.','.'],\n","                                ['.','.','.'],\n","                                ['.','.','.']])\n","        render_str = \"\"\n","        for i in range(self.num_actions):\n","            x, y = np.divmod(i, 3)\n","            if self.present_state[x,y] == 1:\n","                render_state[x,y] = 'X'\n","            elif self.present_state[x,y] == -1:\n","                render_state[x,y] = 'O'\n","\n","            render_str += f\" {render_state[x,y]}\"\n","            if (i+1) % 3 == 0:\n","                render_str += \"\\n\" + \"-\"*11 + \"\\n\"\n","            else:\n","                render_str += \" |\"\n","\n","        print(render_str)\n","\n","\n","    def check_available_action(self, state):\n","        '''\n","        현재 state에서 가능한 actions array 반환\n","        원핫 인코딩 방식으로 구현\n","        '''\n","        impossible_actions = np.argwhere(state.reshape(-1) != 0)\n","        available_actions = np.ones(self.num_actions)\n","        available_actions[impossible_actions] = 0\n","\n","        return available_actions\n","\n","\n","    def is_done(self, state):\n","        '''\n","        틱택토 게임 종료 조건 및 승리 여부 확인하는 함수\n","        '''\n","        is_done, is_win = False, \"null\"\n","\n","        # 무승부 여부 확인\n","        if (state==0).sum()==0:\n","            is_done, is_win = True, \"draw\"\n","\n","        else:\n","            axis_sum = np.concatenate((state.sum(axis=0), state.sum(axis=1)))\n","            diag_sum = np.array([state.trace(), np.fliplr(state).trace()])\n","\n","            sum_array = np.concatenate((axis_sum, diag_sum))\n","            max_sum = np.max(sum_array)\n","            min_sum = np.min(sum_array)\n","\n","            if max_sum == 3:\n","                is_done, is_win = True, \"win\"\n","            elif min_sum == -3:\n","                is_done, is_win = True, \"lose\"\n","            else:\n","                is_done, is_win = False, \"null\"\n","\n","        return is_done, is_win"],"metadata":{"id":"-Xb7nqY8jisk","executionInfo":{"status":"ok","timestamp":1728286166170,"user_tz":-540,"elapsed":522,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["# MTCS 에이전트"],"metadata":{"id":"0HDRLbbFdd7t"}},{"cell_type":"markdown","source":["- 헷갈려... 일단 밖에 빼서 따로 시험해봄"],"metadata":{"id":"PjXSyXUvz9ZY"}},{"cell_type":"code","source":["class Agent:\n","    def __init__(self, env, player:bool):\n","        self.env = env\n","        self.player = player\n","\n","        self.n = self.env.n\n","        self.num_actions = self.env.num_actions\n","        self.actions = self.env.action_space\n","\n","        self.stepsize = STEPSIZE\n","        self.gamma = GAMMA\n","        self.epsilon = EPSILON\n","        self.epsilon_decay = EPSILON_DECAY\n","        self.epsilon_min = EPSILON_MIN\n","\n","\n","    def UCT(self, environment, itermax, player):\n","        root_state = env.present_state\n","        root_node = Node(env=env, state=root_state, player = player)\n","\n","        for _ in range(itermax):\n","            node = root_node\n","            state = copy.deepcopy(root_state)\n","            env = copy.deepcopy(environment)\n","\n","            # selection\n","            while (node.available_actions.sum() > 0) and node.childnode_list != []:\n","                node = node.select_child()\n","                state, _, _, _ = env.step(node.action, node.player)\n","\n","            # expansion\n","            if node.available_actions.sum() > 0:\n","                available_action = np.where(node.available_actions != 0)[0]\n","\n","                action = np.random.choice(available_action)\n","                state, _, _, _ = env.step(action, node.player)\n","                node = node.add_child(action, state)\n","\n","            # simulation\n","            while not env.done:\n","                available_action = np.where(env.available_actions != 0)[0]\n","\n","                action = np.random.choice(available_action)\n","                state, _, _, _ = env.step(action, node.player)\n","\n","            # backpropagation\n","            while node.parent_node != None:\n","                result = env.is_done(state)\n","                node.update(result)\n","                node = node.parent_node\n","\n","        # 최적의 행동 선택\n","        s = sorted(root_node.childnode_list, key=lambda c: c.wins / c.visits)\n","\n","        return s[-1].action\n","\n","\n","    def update_value_table(self, history):\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","        else:\n","            self.epsilon = self.epsilon_min\n","\n","        G = 0\n","        for hist in reversed(history):\n","            action_idx, reward = hist\n","\n","            G = self.gamma * G + reward\n","            self.returns[action_idx].append(G)\n","            self.value_table[action_idx] = np.mean(self.returns[action_idx])\n","\n","\n","    def get_action(self, state, available_actions):\n","        available_action = np.where(available_actions != 0)[0]\n","        if (np.random.rand() <= self.epsilon) or (not self.player):\n","            action = np.random.choice(available_action)\n","\n","        else:\n","            available_value = self.value_table * available_actions\n","            action = np.argmax(available_value)\n","\n","        return action"],"metadata":{"id":"sxvrWiCYdgJW","executionInfo":{"status":"ok","timestamp":1728286166713,"user_tz":-540,"elapsed":2,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["# Node 클래스, UCT 함수"],"metadata":{"id":"DieL4ftB0CUQ"}},{"cell_type":"code","source":["class Node():\n","    def __init__(self, env, action = None, state = None, parent = None, player = True):\n","        self.env = copy.deepcopy(env)\n","        self.action = action\n","        self.player = player\n","        self.available_actions = self.env.check_available_action(state)\n","        self.parent_node = parent\n","        self.childnode_list = []\n","\n","        self.visits = 0\n","        self.wins = 0\n","\n","\n","    def select_child(self):\n","        '''\n","        UCT를 이용한 child node 선택 (selection에 해당)\n","        '''\n","        # UTC 공식을 적용하여 값이 낮은 것부터 child node들을 정렬 (마지막이 최댓값을 갖는 노드)\n","        s = sorted(self.childnode_list, key=lambda c: c.wins / c.visits + np.sqrt(2 * np.log(self.visits) / c.visits))\n","        return s[-1]\n","\n","\n","    def add_child(self, action, state):\n","        '''\n","        action에 해당하는 child node 추가 (expansion에 해당)\n","        '''\n","        n = Node(self.env, action, copy.deepcopy(state), parent=self, player = not self.player)\n","        self.available_actions[action] = 0\n","        self.childnode_list.append(n)\n","        return n\n","\n","    def update(self, result):\n","        '''\n","        이 노드의 값 업데이트 (backpropagation에 해당)\n","        '''\n","        self.visits += 1\n","        self.wins += result"],"metadata":{"id":"Ji1Uaz4dS4JF","executionInfo":{"status":"ok","timestamp":1728286167749,"user_tz":-540,"elapsed":507,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def UCT(environment, itermax, player):\n","    root_state = environment.present_state\n","    root_node = Node(env=environment, state=root_state, player = player)\n","\n","    for _ in range(itermax):\n","        node = root_node\n","        state = copy.deepcopy(root_state)\n","        env = copy.deepcopy(environment)\n","\n","        # selection\n","        while (node.available_actions.sum() > 0) and node.childnode_list != []:\n","            node = node.select_child()\n","            state, _, _, _ = env.step(node.action, node.player)\n","\n","        # expansion\n","        if node.available_actions.sum() > 0:\n","            available_action = np.where(node.available_actions != 0)[0]\n","\n","            action = np.random.choice(available_action)\n","            state, _, _, _ = env.step(action, node.player)\n","            node = node.add_child(action, state)\n","\n","        # simulation\n","        while not env.done:\n","            available_action = np.where(env.available_actions != 0)[0]\n","\n","            action = np.random.choice(available_action)\n","            state, _, _, _ = env.step(action, node.player)\n","\n","        # backpropagation\n","        while node.parent_node != None:\n","            is_done, is_win = env.is_done(state)\n","            result = env.reward_dict[is_win]\n","            node.update(result)\n","            node = node.parent_node\n","\n","    # 최적의 행동 선택\n","    s = sorted(root_node.childnode_list, key=lambda c: c.wins / c.visits)\n","\n","    return s[-1].action"],"metadata":{"id":"gAMAJUbyroxE","executionInfo":{"status":"ok","timestamp":1728286241682,"user_tz":-540,"elapsed":392,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["## UCT main"],"metadata":{"id":"AdIpnM5R0GhO"}},{"cell_type":"code","source":["MAXITER = 10000\n","env = Environment()"],"metadata":{"id":"hwWdJw_505zl","executionInfo":{"status":"ok","timestamp":1728286243777,"user_tz":-540,"elapsed":444,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["def UCTPlayGame(first_player:bool):\n","    env = Environment()\n","    state = env.present_state\n","    done = env.done\n","    player = first_player\n","    is_win = \"null\"\n","\n","    env.render()\n","\n","    while not done:\n","        print(f\"player:{player}\")\n","        if player:\n","            action = UCT(env, MAXITER, player)\n","\n","        else:\n","            available_action = np.where(env.available_actions != 0)[0]\n","            action = np.random.choice(available_action)\n","\n","        state, _, done, is_win = env.step(action, player)\n","        player = not player\n","\n","        env.render()\n","        print(f\"win:{is_win}\")"],"metadata":{"id":"bzpzec0G0IE-","executionInfo":{"status":"ok","timestamp":1728286244302,"user_tz":-540,"elapsed":3,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["UCTPlayGame(True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3i3--PbF2M6E","executionInfo":{"status":"ok","timestamp":1728286267967,"user_tz":-540,"elapsed":22528,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}},"outputId":"af66a433-b9dd-4d12-9ca3-79b629ad1e28"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":[" . | . | .\n","-----------\n"," . | . | .\n","-----------\n"," . | . | .\n","-----------\n","\n","player:True\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-49-2e8d5ee94b47>:19: RuntimeWarning: divide by zero encountered in log\n","  s = sorted(self.childnode_list, key=lambda c: c.wins / c.visits + np.sqrt(2 * np.log(self.visits) / c.visits))\n","<ipython-input-49-2e8d5ee94b47>:19: RuntimeWarning: invalid value encountered in sqrt\n","  s = sorted(self.childnode_list, key=lambda c: c.wins / c.visits + np.sqrt(2 * np.log(self.visits) / c.visits))\n"]},{"output_type":"stream","name":"stdout","text":[" . | . | .\n","-----------\n"," . | . | .\n","-----------\n"," . | X | .\n","-----------\n","\n","win:null\n","player:False\n"," O | . | .\n","-----------\n"," . | . | .\n","-----------\n"," . | X | .\n","-----------\n","\n","win:null\n","player:True\n"," O | . | .\n","-----------\n"," . | . | .\n","-----------\n"," X | X | .\n","-----------\n","\n","win:null\n","player:False\n"," O | . | .\n","-----------\n"," O | . | .\n","-----------\n"," X | X | .\n","-----------\n","\n","win:null\n","player:True\n"," O | X | .\n","-----------\n"," O | . | .\n","-----------\n"," X | X | .\n","-----------\n","\n","win:null\n","player:False\n"," O | X | .\n","-----------\n"," O | . | O\n","-----------\n"," X | X | .\n","-----------\n","\n","win:null\n","player:True\n"," O | X | .\n","-----------\n"," O | . | O\n","-----------\n"," X | X | X\n","-----------\n","\n","win:win\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"U-6ltRGF4LDa"},"execution_count":null,"outputs":[]}]}