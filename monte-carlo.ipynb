{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["vgzsYaLOjm5p"],"authorship_tag":"ABX9TyM+7eC+ZOvAsp2BgIW1qlR4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- 일단 중복 고려 안함. 이동 시 발생하는 reward 0.\n","- 오직 게임이 끝나고 win, lose, draw에 따라서만 reward 발생\n","- 몬테카를로 구현~~"],"metadata":{"id":"NEQfh9B2ofMw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWFBBg-ZjXOf"},"outputs":[],"source":["import time\n","import os\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from typing import Tuple\n","from collections import deque\n","import copy\n","from scipy.special import softmax\n","import random\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"markdown","source":["# 틱택토 환경"],"metadata":{"id":"nZPH06v7jkS3"}},{"cell_type":"code","source":["class Environment:\n","    def __init__(self):\n","        self.n = 3\n","        self.num_actions = self.n**2\n","        self.present_state = np.zeros((self.n, self.n))\n","        self.action_space = np.arange(self.num_actions)\n","        self.reward_dict = {'win':1, 'lose':-1, 'draw': -0.1, 'good_action':0, 'overlapped':0}\n","        self.done = False\n","\n","\n","    def step(self, action_idx:int, max_player:bool):\n","        '''\n","        에이전트가 선택한 action에 따라 주어지는 next_state, reward, done\n","        '''\n","        x, y = np.divmod(action_idx, self.n)\n","\n","        is_overlap = self.is_overlap(action_idx)\n","\n","        if is_overlap:\n","            next_state = self.present_state\n","            reward = self.reward_dict['overlapped']\n","            done, is_win = self.is_done(next_state)\n","\n","        else:\n","            self.present_state[x,y] = max_player*2 -1\n","            next_state = self.present_state\n","            done, is_win = self.is_done(next_state)\n","            reward = self.reward_dict['good_action']\n","\n","            if done:\n","                if is_win == \"win\":\n","                    reward = self.reward_dict['win']\n","                elif is_win == \"lose\":\n","                    reward = self.reward_dict['lose']\n","                else:\n","                    reward = self.reward_dict['draw']\n","\n","        self.done = done\n","\n","        return next_state, reward, done, is_win\n","\n","\n","    def reset(self):\n","        '''\n","        게임판 초기화\n","        '''\n","        self.present_state = np.zeros((self.n, self.n))\n","        self.done = False\n","\n","\n","    def render(self):\n","        '''\n","        print the current state\n","        '''\n","        render_state = np.array([['.','.','.'],\n","                                ['.','.','.'],\n","                                ['.','.','.']])\n","        render_str = \"\"\n","        for i in range(self.num_actions):\n","            x, y = np.divmod(i, 3)\n","            if self.present_state[x,y] == 1:\n","                render_state[x,y] = 'X'\n","            elif self.present_state[x,y] == -1:\n","                render_state[x,y] = 'O'\n","\n","            render_str += f\" {render_state[x,y]}\"\n","            if (i+1) % 3 == 0:\n","                render_str += \"\\n\" + \"-\"*11 + \"\\n\"\n","            else:\n","                render_str += \" |\"\n","\n","        print(render_str)\n","\n","\n","    def is_overlap(self, action_idx):\n","        '''\n","        action이 중복인지 판단하는 함수\n","        '''\n","        is_overlap = False\n","        x, y = np.divmod(action_idx, self.n)\n","        if self.present_state[x,y] != 0:\n","            is_overlap = True\n","\n","        return is_overlap\n","\n","\n","    def is_done(self, state):\n","        '''\n","        틱택토 게임 종료 조건 및 승리 여부 확인하는 함수\n","        '''\n","        is_done, is_win = False, \"null\"\n","\n","        # 무승부 여부 확인\n","        if (state==0).sum()==0:\n","            is_done, is_win = True, \"draw\"\n","\n","        else:\n","            axis_sum = np.concatenate((state.sum(axis=0), state.sum(axis=1)))\n","            diag_sum = np.array([state.trace(), np.fliplr(state).trace()])\n","\n","            sum_array = np.concatenate((axis_sum, diag_sum))\n","            max_sum = np.max(sum_array)\n","            min_sum = np.min(sum_array)\n","\n","            if max_sum == 3:\n","                is_done, is_win = True, \"win\"\n","            elif min_sum == -3:\n","                is_done, is_win = True, \"lose\"\n","            else:\n","                is_done, is_win = False, \"null\"\n","\n","        return is_done, is_win"],"metadata":{"id":"-Xb7nqY8jisk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 몬테카를로 알고리즘\n","- 랜덤 탐색 -> 가치함수 업데이트"],"metadata":{"id":"0HDRLbbFdd7t"}},{"cell_type":"code","source":[],"metadata":{"id":"sxvrWiCYdgJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# minimax 알고리즘\n","- 입력 받은 상태에서 얻을 수 있는 최대값이 뭔지 알려주는 함수\n","- 개선점: 최대값을 얻을 수 있는 행동이 무엇인지 반환해야한다.\n","- 현재 모든 경우의 수에 대해 계산하는 minimax 함수이다. 틱택토 정도의 작은 상황에서는 가능하지만, 상태의 수가 많아지면 depth를 도입해 일정 깊이만큼만 탐색하도록 해야한다."],"metadata":{"id":"vgzsYaLOjm5p"}},{"cell_type":"code","source":["class Agent:\n","    def __init__(self, env, max_player:bool):\n","        self.env = env\n","        self.n = self.env.n\n","        self.num_actions = self.env.num_actions\n","        self.actions = torch.tensor(self.num_actions)\n","\n","        self.best_action = None\n","\n","\n","    def minimax(self, present_state, depth, alpha, beta, max_player:bool):\n","        temp_env = Environment()\n","        state = copy.deepcopy(present_state)\n","        temp_env.present_state = state\n","        done, is_win = temp_env.is_done(state)\n","        reward = 0\n","\n","        remain_actions = np.argwhere(state == 0)\n","\n","\n","        if (done == True) or (depth == 0):\n","            if is_win == \"win\":\n","                reward = temp_env.reward_dict['win']\n","\n","            elif is_win == \"lose\":\n","                reward = temp_env.reward_dict['lose']\n","\n","            else:\n","                reward = temp_env.reward_dict['draw']\n","\n","            return reward\n","\n","\n","        if max_player:\n","            maxEval = -np.Inf\n","            best_action = None\n","            for (x, y) in remain_actions:\n","                idx = self.n * x + y\n","                child, _, _, _ = temp_env.step(idx, True)\n","\n","                eval = self.minimax(child, depth-1, alpha, beta, False)\n","\n","                if eval > maxEval:\n","                    best_action = idx\n","                    maxEval = eval\n","\n","                alpha = max(alpha, eval)\n","                if beta <= alpha:\n","                    break\n","\n","                if depth == DEPTH:  # 최상위 호출에서만 best_action을 저장\n","                    self.best_action = best_action\n","\n","            return maxEval\n","\n","        else:\n","            minEval = np.Inf\n","            for (x, y) in remain_actions:\n","                idx = self.n * x + y\n","                child, _, _, _ = temp_env.step(idx, False)\n","\n","                eval = self.minimax(child, depth-1, alpha, beta, True)\n","                minEval = min(minEval, eval)\n","\n","                beta = min(alpha, eval)\n","                if beta <= alpha:\n","                    break\n","\n","            return minEval\n","\n","\n","    def get_action(self, state, agent_turn):\n","        self.minimax(state, DEPTH, -np.Inf, np.Inf, True)\n","        return self.best_action"],"metadata":{"id":"rugve7NAjsAo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEPTH = 100"],"metadata":{"id":"bCdlr38KXMV0"},"execution_count":null,"outputs":[]}]}