{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOIShHS4nLWY8DDTQAMSzy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- 일단 중복 고려 안함. 이동 시 발생하는 reward 0.\n","- 오직 게임이 끝나고 win, lose, draw에 따라서만 reward 발생\n","- 몬테카를로 구현~~"],"metadata":{"id":"NEQfh9B2ofMw"}},{"cell_type":"code","execution_count":24,"metadata":{"id":"KWFBBg-ZjXOf","executionInfo":{"status":"ok","timestamp":1727688117267,"user_tz":-540,"elapsed":1048,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"outputs":[],"source":["import time\n","import os\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from typing import Tuple\n","from collections import deque\n","import copy\n","from scipy.special import softmax\n","import random\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"markdown","source":["# 틱택토 환경"],"metadata":{"id":"nZPH06v7jkS3"}},{"cell_type":"code","source":["class Environment:\n","    def __init__(self):\n","        self.n = 3\n","        self.num_actions = self.n**2\n","        self.present_state = np.zeros((self.n, self.n))\n","        self.action_space = np.arange(self.num_actions)\n","        self.available_actions = np.ones(self.num_actions)\n","        self.reward_dict = {'win':1, 'lose':-1, 'draw': -0.1, 'good_action':0, 'overlapped':0}\n","        self.done = False\n","\n","\n","    def step(self, action_idx:int, max_player:bool):\n","        '''\n","        에이전트가 선택한 action에 따라 주어지는 next_state, reward, done\n","        '''\n","        x, y = np.divmod(action_idx, self.n)\n","\n","        self.present_state[x,y] = max_player*2 -1\n","        next_state = self.present_state\n","        done, is_win = self.is_done(next_state)\n","        reward = self.reward_dict['good_action']\n","        self.available_actions = self.check_available_action(self.present_state)\n","\n","        if done:\n","            if is_win == \"win\":\n","                reward = self.reward_dict['win']\n","            elif is_win == \"lose\":\n","                reward = self.reward_dict['lose']\n","            else:\n","                reward = self.reward_dict['draw']\n","\n","        self.done = done\n","\n","        return next_state, reward, done, is_win\n","\n","\n","    def reset(self):\n","        '''\n","        게임판 초기화\n","        '''\n","        self.present_state = np.zeros((self.n, self.n))\n","        self.available_actions = np.ones(self.num_actions)\n","        self.done = False\n","\n","\n","    def render(self):\n","        '''\n","        print the current state\n","        '''\n","        render_state = np.array([['.','.','.'],\n","                                ['.','.','.'],\n","                                ['.','.','.']])\n","        render_str = \"\"\n","        for i in range(self.num_actions):\n","            x, y = np.divmod(i, 3)\n","            if self.present_state[x,y] == 1:\n","                render_state[x,y] = 'X'\n","            elif self.present_state[x,y] == -1:\n","                render_state[x,y] = 'O'\n","\n","            render_str += f\" {render_state[x,y]}\"\n","            if (i+1) % 3 == 0:\n","                render_str += \"\\n\" + \"-\"*11 + \"\\n\"\n","            else:\n","                render_str += \" |\"\n","\n","        print(render_str)\n","\n","\n","    def check_available_action(self, state):\n","        '''\n","        현재 state에서 가능한 actions array 반환\n","        '''\n","        impossible_actions = np.argwhere(state.reshape(-1) != 0)\n","        available_actions = np.ones(self.num_actions)\n","        available_actions[impossible_actions] = 0\n","\n","        return available_actions\n","\n","\n","    def is_done(self, state):\n","        '''\n","        틱택토 게임 종료 조건 및 승리 여부 확인하는 함수\n","        '''\n","        is_done, is_win = False, \"null\"\n","\n","        # 무승부 여부 확인\n","        if (state==0).sum()==0:\n","            is_done, is_win = True, \"draw\"\n","\n","        else:\n","            axis_sum = np.concatenate((state.sum(axis=0), state.sum(axis=1)))\n","            diag_sum = np.array([state.trace(), np.fliplr(state).trace()])\n","\n","            sum_array = np.concatenate((axis_sum, diag_sum))\n","            max_sum = np.max(sum_array)\n","            min_sum = np.min(sum_array)\n","\n","            if max_sum == 3:\n","                is_done, is_win = True, \"win\"\n","            elif min_sum == -3:\n","                is_done, is_win = True, \"lose\"\n","            else:\n","                is_done, is_win = False, \"null\"\n","\n","        return is_done, is_win"],"metadata":{"id":"-Xb7nqY8jisk","executionInfo":{"status":"ok","timestamp":1727688804080,"user_tz":-540,"elapsed":327,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":["# 몬테카를로 알고리즘\n","- 랜덤 탐색 -> 가치함수 업데이트"],"metadata":{"id":"0HDRLbbFdd7t"}},{"cell_type":"code","source":["class Agent:\n","    def __init__(self, env, max_player:bool):\n","        self.env = env\n","        self.max_player = max_player\n","\n","        self.n = self.env.n\n","        self.num_actions = self.env.num_actions\n","        self.actions = self.env.action_space\n","\n","        self.value_table = np.zeros(self.num_actions)\n","        self.returns = {i: [] for i in range(self.num_actions)}\n","\n","        self.stepsize = STEPSIZE\n","        self.gamma = GAMMA\n","        self.epsilon = EPSILON\n","        self.epsilon_decay = EPSILON_DECAY\n","        self.epsilon_min = EPSILON_MIN\n","\n","\n","    def update_value_table(self, history):\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","        G = 0\n","        for t in reversed(range(len(history))):\n","            action_idx, reward = history[t]\n","\n","            G = self.gamma * G + reward\n","            self.returns[action_idx].append(G)\n","            self.value_table[action_idx] = np.mean(self.returns[action_idx])\n","\n","\n","    def get_action(self, state, available_actions):\n","        available_action = np.where(available_actions != 0)[0]\n","\n","        if (np.random.rand() <= self.epsilon) or (not self.max_player):\n","            act = random.choice(available_action)\n","\n","        else:\n","            available_value = self.value_table * available_actions\n","            act = np.argmax(available_value)\n","\n","        return act"],"metadata":{"id":"sxvrWiCYdgJW","executionInfo":{"status":"ok","timestamp":1727688805577,"user_tz":-540,"elapsed":309,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["## main"],"metadata":{"id":"-iDjv1fhpsYx"}},{"cell_type":"code","source":["STEPSIZE = 0.1\n","GAMMA = 0.9\n","EPSILON = 0.999\n","EPSILON_DECAY = 0.999\n","EPSILON_MIN = 0.01\n","EPISODES = 1000"],"metadata":{"id":"KrvTXp1XEauz","executionInfo":{"status":"ok","timestamp":1727688843789,"user_tz":-540,"elapsed":301,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["env = Environment()\n","agent = Agent(env, True)\n","player = Agent(env, False)\n","env.reset()"],"metadata":{"id":"P7rCdI_Mprak","executionInfo":{"status":"ok","timestamp":1727688844132,"user_tz":-540,"elapsed":1,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["for episode in range(EPISODES):\n","    env.reset()\n","    state = env.present_state\n","    done = env.done\n","\n","    history = []\n","\n","    while not done:\n","        agent_action = agent.get_action(state, env.available_actions)\n","        next_state, reward, done, is_win = env.step(agent_action, True)\n","        history.append((agent_action, reward))\n","\n","        if not done:\n","            player_action = player.get_action(next_state, env.available_actions)\n","            next_state, reward, done, is_win = env.step(player_action, False)\n","\n","        state = next_state\n","\n","    agent.update_value_table(history)\n","\n","    if (episode+1) % 10 == 0:\n","        print(f\"Episode: {episode+1}, win?: {is_win}\")\n","        env.render()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1bstnPRqq-z","executionInfo":{"status":"ok","timestamp":1727688867457,"user_tz":-540,"elapsed":1660,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}},"outputId":"ae6c2b7c-a48f-4f72-fb29-21d04ddc0223"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 10, win?: lose\n"," X | O | X\n","-----------\n"," . | O | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 20, win?: lose\n"," X | O | X\n","-----------\n"," . | X | X\n","-----------\n"," O | O | O\n","-----------\n","\n","Episode: 30, win?: lose\n"," X | . | .\n","-----------\n"," X | X | .\n","-----------\n"," O | O | O\n","-----------\n","\n","Episode: 40, win?: draw\n"," O | X | X\n","-----------\n"," X | O | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 50, win?: lose\n"," O | O | O\n","-----------\n"," . | X | .\n","-----------\n"," . | X | X\n","-----------\n","\n","Episode: 60, win?: lose\n"," O | O | O\n","-----------\n"," . | . | X\n","-----------\n"," . | X | X\n","-----------\n","\n","Episode: 70, win?: win\n"," O | . | .\n","-----------\n"," X | X | X\n","-----------\n"," . | O | .\n","-----------\n","\n","Episode: 80, win?: win\n"," X | . | .\n","-----------\n"," O | X | O\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 90, win?: win\n"," . | X | .\n","-----------\n"," O | X | X\n","-----------\n"," O | X | O\n","-----------\n","\n","Episode: 100, win?: win\n"," . | X | X\n","-----------\n"," O | X | O\n","-----------\n"," X | . | O\n","-----------\n","\n","Episode: 110, win?: win\n"," X | O | .\n","-----------\n"," X | X | X\n","-----------\n"," O | O | .\n","-----------\n","\n","Episode: 120, win?: draw\n"," X | X | X\n","-----------\n"," O | O | X\n","-----------\n"," X | O | O\n","-----------\n","\n","Episode: 130, win?: win\n"," . | . | .\n","-----------\n"," X | X | X\n","-----------\n"," O | . | O\n","-----------\n","\n","Episode: 140, win?: draw\n"," X | O | O\n","-----------\n"," X | X | X\n","-----------\n"," O | X | O\n","-----------\n","\n","Episode: 150, win?: win\n"," . | X | .\n","-----------\n"," O | X | O\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 160, win?: win\n"," X | O | .\n","-----------\n"," . | X | X\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 170, win?: win\n"," . | . | O\n","-----------\n"," X | X | X\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 180, win?: draw\n"," X | X | O\n","-----------\n"," O | X | X\n","-----------\n"," X | O | O\n","-----------\n","\n","Episode: 190, win?: lose\n"," O | O | O\n","-----------\n"," X | X | .\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 200, win?: draw\n"," O | X | O\n","-----------\n"," O | X | X\n","-----------\n"," X | X | O\n","-----------\n","\n","Episode: 210, win?: draw\n"," O | X | O\n","-----------\n"," O | X | X\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 220, win?: win\n"," X | . | .\n","-----------\n"," . | X | O\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 230, win?: win\n"," X | . | X\n","-----------\n"," . | X | O\n","-----------\n"," X | O | O\n","-----------\n","\n","Episode: 240, win?: draw\n"," X | O | X\n","-----------\n"," O | X | O\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 250, win?: win\n"," X | . | .\n","-----------\n"," O | X | O\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 260, win?: win\n"," X | O | .\n","-----------\n"," O | X | .\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 270, win?: draw\n"," X | O | O\n","-----------\n"," O | X | X\n","-----------\n"," X | X | O\n","-----------\n","\n","Episode: 280, win?: draw\n"," X | O | X\n","-----------\n"," O | X | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 290, win?: lose\n"," X | O | O\n","-----------\n"," . | O | X\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 300, win?: draw\n"," X | O | X\n","-----------\n"," X | X | O\n","-----------\n"," O | X | O\n","-----------\n","\n","Episode: 310, win?: win\n"," X | O | .\n","-----------\n"," X | X | .\n","-----------\n"," X | O | O\n","-----------\n","\n","Episode: 320, win?: win\n"," X | O | .\n","-----------\n"," . | X | O\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 330, win?: draw\n"," X | O | X\n","-----------\n"," O | X | O\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 340, win?: win\n"," X | O | O\n","-----------\n"," X | X | .\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 350, win?: win\n"," X | . | .\n","-----------\n"," O | X | X\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 360, win?: draw\n"," X | X | O\n","-----------\n"," O | X | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 370, win?: draw\n"," X | X | O\n","-----------\n"," O | X | X\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 380, win?: draw\n"," O | O | X\n","-----------\n"," O | X | X\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 390, win?: lose\n"," X | O | X\n","-----------\n"," . | X | X\n","-----------\n"," O | O | O\n","-----------\n","\n","Episode: 400, win?: win\n"," O | . | X\n","-----------\n"," . | X | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 410, win?: lose\n"," O | O | O\n","-----------\n"," O | X | X\n","-----------\n"," X | . | X\n","-----------\n","\n","Episode: 420, win?: win\n"," O | O | X\n","-----------\n"," . | X | O\n","-----------\n"," X | . | X\n","-----------\n","\n","Episode: 430, win?: win\n"," X | O | .\n","-----------\n"," . | X | .\n","-----------\n"," O | . | X\n","-----------\n","\n","Episode: 440, win?: win\n"," X | . | O\n","-----------\n"," . | X | O\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 450, win?: win\n"," X | . | O\n","-----------\n"," O | X | .\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 460, win?: win\n"," X | O | .\n","-----------\n"," . | X | O\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 470, win?: win\n"," O | . | X\n","-----------\n"," . | O | X\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 480, win?: win\n"," X | X | X\n","-----------\n"," . | . | O\n","-----------\n"," O | . | .\n","-----------\n","\n","Episode: 490, win?: win\n"," X | O | .\n","-----------\n"," . | X | .\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 500, win?: win\n"," X | O | .\n","-----------\n"," O | X | .\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 510, win?: lose\n"," X | O | X\n","-----------\n"," . | X | X\n","-----------\n"," O | O | O\n","-----------\n","\n","Episode: 520, win?: win\n"," X | X | X\n","-----------\n"," X | O | .\n","-----------\n"," O | . | O\n","-----------\n","\n","Episode: 530, win?: draw\n"," X | X | X\n","-----------\n"," X | O | O\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 540, win?: draw\n"," X | X | O\n","-----------\n"," X | X | O\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 550, win?: draw\n"," X | O | X\n","-----------\n"," X | O | X\n","-----------\n"," O | X | O\n","-----------\n","\n","Episode: 560, win?: win\n"," X | X | X\n","-----------\n"," . | O | .\n","-----------\n"," . | O | .\n","-----------\n","\n","Episode: 570, win?: win\n"," O | . | X\n","-----------\n"," O | X | .\n","-----------\n"," X | X | O\n","-----------\n","\n","Episode: 580, win?: win\n"," X | X | O\n","-----------\n"," O | X | .\n","-----------\n"," . | X | O\n","-----------\n","\n","Episode: 590, win?: lose\n"," X | O | X\n","-----------\n"," . | O | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 600, win?: lose\n"," X | . | .\n","-----------\n"," . | X | X\n","-----------\n"," O | O | O\n","-----------\n","\n","Episode: 610, win?: draw\n"," O | O | X\n","-----------\n"," X | O | O\n","-----------\n"," X | X | X\n","-----------\n","\n","Episode: 620, win?: win\n"," X | O | O\n","-----------\n"," X | X | .\n","-----------\n"," O | . | X\n","-----------\n","\n","Episode: 630, win?: win\n"," X | . | X\n","-----------\n"," . | X | O\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 640, win?: win\n"," X | . | .\n","-----------\n"," O | X | .\n","-----------\n"," O | . | X\n","-----------\n","\n","Episode: 650, win?: draw\n"," O | X | O\n","-----------\n"," X | X | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 660, win?: lose\n"," X | . | O\n","-----------\n"," X | O | X\n","-----------\n"," O | X | O\n","-----------\n","\n","Episode: 670, win?: lose\n"," O | X | X\n","-----------\n"," . | X | X\n","-----------\n"," O | O | O\n","-----------\n","\n","Episode: 680, win?: lose\n"," O | O | O\n","-----------\n"," . | X | .\n","-----------\n"," X | . | X\n","-----------\n","\n","Episode: 690, win?: win\n"," O | . | X\n","-----------\n"," . | X | O\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 700, win?: win\n"," O | O | X\n","-----------\n"," O | X | .\n","-----------\n"," X | . | X\n","-----------\n","\n","Episode: 710, win?: win\n"," X | X | O\n","-----------\n"," . | X | O\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 720, win?: draw\n"," X | O | X\n","-----------\n"," X | X | O\n","-----------\n"," X | O | O\n","-----------\n","\n","Episode: 730, win?: draw\n"," X | X | O\n","-----------\n"," O | O | X\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 740, win?: draw\n"," X | O | X\n","-----------\n"," X | O | O\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 750, win?: win\n"," X | . | O\n","-----------\n"," O | X | .\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 760, win?: win\n"," X | X | .\n","-----------\n"," . | X | O\n","-----------\n"," O | X | O\n","-----------\n","\n","Episode: 770, win?: draw\n"," O | O | X\n","-----------\n"," X | X | O\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 780, win?: win\n"," X | O | .\n","-----------\n"," . | X | .\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 790, win?: draw\n"," O | X | X\n","-----------\n"," X | X | O\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 800, win?: win\n"," X | . | .\n","-----------\n"," . | X | O\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 810, win?: draw\n"," X | O | X\n","-----------\n"," O | X | X\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 820, win?: win\n"," X | . | O\n","-----------\n"," . | X | .\n","-----------\n"," O | . | X\n","-----------\n","\n","Episode: 830, win?: lose\n"," O | O | O\n","-----------\n"," X | X | .\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 840, win?: lose\n"," X | O | X\n","-----------\n"," X | X | .\n","-----------\n"," O | O | O\n","-----------\n","\n","Episode: 850, win?: win\n"," X | . | .\n","-----------\n"," O | X | O\n","-----------\n"," . | . | X\n","-----------\n","\n","Episode: 860, win?: lose\n"," X | . | O\n","-----------\n"," . | X | O\n","-----------\n"," . | X | O\n","-----------\n","\n","Episode: 870, win?: win\n"," X | . | O\n","-----------\n"," O | X | O\n","-----------\n"," X | . | X\n","-----------\n","\n","Episode: 880, win?: win\n"," X | O | .\n","-----------\n"," O | X | .\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 890, win?: win\n"," X | O | .\n","-----------\n"," . | X | .\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 900, win?: draw\n"," X | O | O\n","-----------\n"," X | O | O\n","-----------\n"," X | X | X\n","-----------\n","\n","Episode: 910, win?: draw\n"," O | O | X\n","-----------\n"," X | X | X\n","-----------\n"," O | O | X\n","-----------\n","\n","Episode: 920, win?: lose\n"," O | O | O\n","-----------\n"," X | X | .\n","-----------\n"," O | X | X\n","-----------\n","\n","Episode: 930, win?: win\n"," X | X | .\n","-----------\n"," O | X | O\n","-----------\n"," . | O | X\n","-----------\n","\n","Episode: 940, win?: win\n"," X | O | O\n","-----------\n"," . | X | .\n","-----------\n"," X | O | X\n","-----------\n","\n","Episode: 950, win?: draw\n"," X | X | O\n","-----------\n"," O | X | X\n","-----------\n"," X | O | O\n","-----------\n","\n","Episode: 960, win?: lose\n"," X | O | X\n","-----------\n"," . | O | X\n","-----------\n"," . | O | .\n","-----------\n","\n","Episode: 970, win?: lose\n"," O | O | O\n","-----------\n"," O | X | X\n","-----------\n"," X | . | X\n","-----------\n","\n","Episode: 980, win?: win\n"," X | O | X\n","-----------\n"," . | X | .\n","-----------\n"," X | O | O\n","-----------\n","\n","Episode: 990, win?: win\n"," X | X | X\n","-----------\n"," O | . | O\n","-----------\n"," O | . | X\n","-----------\n","\n","Episode: 1000, win?: draw\n"," X | O | O\n","-----------\n"," O | X | X\n","-----------\n"," X | X | O\n","-----------\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lMLbZphlqm8d","executionInfo":{"status":"ok","timestamp":1727668750035,"user_tz":-540,"elapsed":2,"user":{"displayName":"‎이승연(자연과학대학 통계학과)","userId":"16234195072820005349"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# minimax 알고리즘\n","- 입력 받은 상태에서 얻을 수 있는 최대값이 뭔지 알려주는 함수\n","- 개선점: 최대값을 얻을 수 있는 행동이 무엇인지 반환해야한다.\n","- 현재 모든 경우의 수에 대해 계산하는 minimax 함수이다. 틱택토 정도의 작은 상황에서는 가능하지만, 상태의 수가 많아지면 depth를 도입해 일정 깊이만큼만 탐색하도록 해야한다."],"metadata":{"id":"vgzsYaLOjm5p"}},{"cell_type":"code","source":["class Agent:\n","    def __init__(self, env, max_player:bool):\n","        self.env = env\n","        self.n = self.env.n\n","        self.num_actions = self.env.num_actions\n","        self.actions = torch.tensor(self.num_actions)\n","\n","        self.best_action = None\n","\n","\n","    def minimax(self, present_state, depth, alpha, beta, max_player:bool):\n","        temp_env = Environment()\n","        state = copy.deepcopy(present_state)\n","        temp_env.present_state = state\n","        done, is_win = temp_env.is_done(state)\n","        reward = 0\n","\n","        remain_actions = np.argwhere(state == 0)\n","\n","\n","        if (done == True) or (depth == 0):\n","            if is_win == \"win\":\n","                reward = temp_env.reward_dict['win']\n","\n","            elif is_win == \"lose\":\n","                reward = temp_env.reward_dict['lose']\n","\n","            else:\n","                reward = temp_env.reward_dict['draw']\n","\n","            return reward\n","\n","\n","        if max_player:\n","            maxEval = -np.Inf\n","            best_action = None\n","            for (x, y) in remain_actions:\n","                idx = self.n * x + y\n","                child, _, _, _ = temp_env.step(idx, True)\n","\n","                eval = self.minimax(child, depth-1, alpha, beta, False)\n","\n","                if eval > maxEval:\n","                    best_action = idx\n","                    maxEval = eval\n","\n","                alpha = max(alpha, eval)\n","                if beta <= alpha:\n","                    break\n","\n","                if depth == DEPTH:  # 최상위 호출에서만 best_action을 저장\n","                    self.best_action = best_action\n","\n","            return maxEval\n","\n","        else:\n","            minEval = np.Inf\n","            for (x, y) in remain_actions:\n","                idx = self.n * x + y\n","                child, _, _, _ = temp_env.step(idx, False)\n","\n","                eval = self.minimax(child, depth-1, alpha, beta, True)\n","                minEval = min(minEval, eval)\n","\n","                beta = min(alpha, eval)\n","                if beta <= alpha:\n","                    break\n","\n","            return minEval\n","\n","\n","    def get_action(self, state, agent_turn):\n","        self.minimax(state, DEPTH, -np.Inf, np.Inf, True)\n","        return self.best_action"],"metadata":{"id":"rugve7NAjsAo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEPTH = 100"],"metadata":{"id":"bCdlr38KXMV0"},"execution_count":null,"outputs":[]}]}